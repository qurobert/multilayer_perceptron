{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-21T10:37:41.976260Z",
     "start_time": "2025-01-21T10:37:41.953539Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "train_data = pd.read_csv('../data/processed/train_data.csv')\n",
    "\n",
    "train_data"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "       1         2         3         4         5         6         7   \n",
       "0    0.0  0.096928  0.257694  0.103656  0.045387  0.487226  0.373965  \\\n",
       "1    1.0  0.667755  0.570172  0.683505  0.495228  0.554934  0.809214   \n",
       "2    0.0  0.103744  0.140345  0.106489  0.049799  0.221901  0.208975   \n",
       "3    0.0  0.173648  0.524518  0.167369  0.086320  0.396678  0.162444   \n",
       "4    0.0  0.150930  0.174839  0.143459  0.071432  0.548614  0.187811   \n",
       "..   ...       ...       ...       ...       ...       ...       ...   \n",
       "450  0.0  0.090255  0.166723  0.103656  0.042630  0.408053  0.410159   \n",
       "451  0.0  0.220503  0.291512  0.216847  0.114104  0.555836  0.252500   \n",
       "452  0.0  0.345923  0.240446  0.321401  0.207466  0.105263  0.022606   \n",
       "453  1.0  0.331251  0.335137  0.327068  0.193425  0.481809  0.288080   \n",
       "454  0.0  0.246060  0.365573  0.231014  0.133701  0.248262  0.064413   \n",
       "\n",
       "            8         9        10  ...        22        23        24   \n",
       "0    0.733365  0.217445  0.530808  ...  0.084667  0.283316  0.075153  \\\n",
       "1    0.582709  0.743539  0.674242  ...  0.667022  0.571962  0.627970   \n",
       "2    0.140300  0.108350  0.646970  ...  0.073995  0.192164  0.075601   \n",
       "3    0.055740  0.080268  0.422727  ...  0.153682  0.617537  0.137308   \n",
       "4    0.025398  0.064115  0.850000  ...  0.109925  0.144723  0.096867   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "450  0.201640  0.142744  0.425253  ...  0.064141  0.097281  0.060511   \n",
       "451  0.165651  0.173211  0.374242  ...  0.185343  0.459488  0.174810   \n",
       "452  0.016987  0.031064  0.226263  ...  0.248310  0.230011  0.219284   \n",
       "453  0.263824  0.321223  0.307576  ...  0.324084  0.500533  0.316201   \n",
       "454  0.055834  0.087972  0.342929  ...  0.192458  0.554904  0.170178   \n",
       "\n",
       "           25        26        27        28        29        30        31  \n",
       "0    0.034285  0.508684  0.397018  1.000000  0.601375  0.524936  0.409681  \n",
       "1    0.467902  0.514627  0.709327  0.541534  0.997595  0.499310  0.481175  \n",
       "2    0.030697  0.179555  0.136324  0.111581  0.174811  0.338459  0.195855  \n",
       "3    0.066482  0.519910  0.109158  0.089856  0.210859  0.363493  0.173357  \n",
       "4    0.045075  0.371987  0.069244  0.017316  0.088625  0.392667  0.165027  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "450  0.024381  0.327082  0.209865  0.114537  0.164467  0.135817  0.349993  \n",
       "451  0.082703  0.644720  0.231598  0.229473  0.418557  0.244628  0.235668  \n",
       "452  0.122739  0.095754  0.022383  0.030879  0.114536  0.176030  0.040404  \n",
       "453  0.168133  0.595192  0.319692  0.325000  0.627835  0.318155  0.330972  \n",
       "454  0.089117  0.271611  0.059503  0.091454  0.255361  0.222551  0.090122  \n",
       "\n",
       "[455 rows x 31 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.096928</td>\n",
       "      <td>0.257694</td>\n",
       "      <td>0.103656</td>\n",
       "      <td>0.045387</td>\n",
       "      <td>0.487226</td>\n",
       "      <td>0.373965</td>\n",
       "      <td>0.733365</td>\n",
       "      <td>0.217445</td>\n",
       "      <td>0.530808</td>\n",
       "      <td>...</td>\n",
       "      <td>0.084667</td>\n",
       "      <td>0.283316</td>\n",
       "      <td>0.075153</td>\n",
       "      <td>0.034285</td>\n",
       "      <td>0.508684</td>\n",
       "      <td>0.397018</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.601375</td>\n",
       "      <td>0.524936</td>\n",
       "      <td>0.409681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.667755</td>\n",
       "      <td>0.570172</td>\n",
       "      <td>0.683505</td>\n",
       "      <td>0.495228</td>\n",
       "      <td>0.554934</td>\n",
       "      <td>0.809214</td>\n",
       "      <td>0.582709</td>\n",
       "      <td>0.743539</td>\n",
       "      <td>0.674242</td>\n",
       "      <td>...</td>\n",
       "      <td>0.667022</td>\n",
       "      <td>0.571962</td>\n",
       "      <td>0.627970</td>\n",
       "      <td>0.467902</td>\n",
       "      <td>0.514627</td>\n",
       "      <td>0.709327</td>\n",
       "      <td>0.541534</td>\n",
       "      <td>0.997595</td>\n",
       "      <td>0.499310</td>\n",
       "      <td>0.481175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.103744</td>\n",
       "      <td>0.140345</td>\n",
       "      <td>0.106489</td>\n",
       "      <td>0.049799</td>\n",
       "      <td>0.221901</td>\n",
       "      <td>0.208975</td>\n",
       "      <td>0.140300</td>\n",
       "      <td>0.108350</td>\n",
       "      <td>0.646970</td>\n",
       "      <td>...</td>\n",
       "      <td>0.073995</td>\n",
       "      <td>0.192164</td>\n",
       "      <td>0.075601</td>\n",
       "      <td>0.030697</td>\n",
       "      <td>0.179555</td>\n",
       "      <td>0.136324</td>\n",
       "      <td>0.111581</td>\n",
       "      <td>0.174811</td>\n",
       "      <td>0.338459</td>\n",
       "      <td>0.195855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.173648</td>\n",
       "      <td>0.524518</td>\n",
       "      <td>0.167369</td>\n",
       "      <td>0.086320</td>\n",
       "      <td>0.396678</td>\n",
       "      <td>0.162444</td>\n",
       "      <td>0.055740</td>\n",
       "      <td>0.080268</td>\n",
       "      <td>0.422727</td>\n",
       "      <td>...</td>\n",
       "      <td>0.153682</td>\n",
       "      <td>0.617537</td>\n",
       "      <td>0.137308</td>\n",
       "      <td>0.066482</td>\n",
       "      <td>0.519910</td>\n",
       "      <td>0.109158</td>\n",
       "      <td>0.089856</td>\n",
       "      <td>0.210859</td>\n",
       "      <td>0.363493</td>\n",
       "      <td>0.173357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.150930</td>\n",
       "      <td>0.174839</td>\n",
       "      <td>0.143459</td>\n",
       "      <td>0.071432</td>\n",
       "      <td>0.548614</td>\n",
       "      <td>0.187811</td>\n",
       "      <td>0.025398</td>\n",
       "      <td>0.064115</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.109925</td>\n",
       "      <td>0.144723</td>\n",
       "      <td>0.096867</td>\n",
       "      <td>0.045075</td>\n",
       "      <td>0.371987</td>\n",
       "      <td>0.069244</td>\n",
       "      <td>0.017316</td>\n",
       "      <td>0.088625</td>\n",
       "      <td>0.392667</td>\n",
       "      <td>0.165027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>450</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.090255</td>\n",
       "      <td>0.166723</td>\n",
       "      <td>0.103656</td>\n",
       "      <td>0.042630</td>\n",
       "      <td>0.408053</td>\n",
       "      <td>0.410159</td>\n",
       "      <td>0.201640</td>\n",
       "      <td>0.142744</td>\n",
       "      <td>0.425253</td>\n",
       "      <td>...</td>\n",
       "      <td>0.064141</td>\n",
       "      <td>0.097281</td>\n",
       "      <td>0.060511</td>\n",
       "      <td>0.024381</td>\n",
       "      <td>0.327082</td>\n",
       "      <td>0.209865</td>\n",
       "      <td>0.114537</td>\n",
       "      <td>0.164467</td>\n",
       "      <td>0.135817</td>\n",
       "      <td>0.349993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>451</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.220503</td>\n",
       "      <td>0.291512</td>\n",
       "      <td>0.216847</td>\n",
       "      <td>0.114104</td>\n",
       "      <td>0.555836</td>\n",
       "      <td>0.252500</td>\n",
       "      <td>0.165651</td>\n",
       "      <td>0.173211</td>\n",
       "      <td>0.374242</td>\n",
       "      <td>...</td>\n",
       "      <td>0.185343</td>\n",
       "      <td>0.459488</td>\n",
       "      <td>0.174810</td>\n",
       "      <td>0.082703</td>\n",
       "      <td>0.644720</td>\n",
       "      <td>0.231598</td>\n",
       "      <td>0.229473</td>\n",
       "      <td>0.418557</td>\n",
       "      <td>0.244628</td>\n",
       "      <td>0.235668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>452</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.345923</td>\n",
       "      <td>0.240446</td>\n",
       "      <td>0.321401</td>\n",
       "      <td>0.207466</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>0.022606</td>\n",
       "      <td>0.016987</td>\n",
       "      <td>0.031064</td>\n",
       "      <td>0.226263</td>\n",
       "      <td>...</td>\n",
       "      <td>0.248310</td>\n",
       "      <td>0.230011</td>\n",
       "      <td>0.219284</td>\n",
       "      <td>0.122739</td>\n",
       "      <td>0.095754</td>\n",
       "      <td>0.022383</td>\n",
       "      <td>0.030879</td>\n",
       "      <td>0.114536</td>\n",
       "      <td>0.176030</td>\n",
       "      <td>0.040404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>453</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.331251</td>\n",
       "      <td>0.335137</td>\n",
       "      <td>0.327068</td>\n",
       "      <td>0.193425</td>\n",
       "      <td>0.481809</td>\n",
       "      <td>0.288080</td>\n",
       "      <td>0.263824</td>\n",
       "      <td>0.321223</td>\n",
       "      <td>0.307576</td>\n",
       "      <td>...</td>\n",
       "      <td>0.324084</td>\n",
       "      <td>0.500533</td>\n",
       "      <td>0.316201</td>\n",
       "      <td>0.168133</td>\n",
       "      <td>0.595192</td>\n",
       "      <td>0.319692</td>\n",
       "      <td>0.325000</td>\n",
       "      <td>0.627835</td>\n",
       "      <td>0.318155</td>\n",
       "      <td>0.330972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>454</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.246060</td>\n",
       "      <td>0.365573</td>\n",
       "      <td>0.231014</td>\n",
       "      <td>0.133701</td>\n",
       "      <td>0.248262</td>\n",
       "      <td>0.064413</td>\n",
       "      <td>0.055834</td>\n",
       "      <td>0.087972</td>\n",
       "      <td>0.342929</td>\n",
       "      <td>...</td>\n",
       "      <td>0.192458</td>\n",
       "      <td>0.554904</td>\n",
       "      <td>0.170178</td>\n",
       "      <td>0.089117</td>\n",
       "      <td>0.271611</td>\n",
       "      <td>0.059503</td>\n",
       "      <td>0.091454</td>\n",
       "      <td>0.255361</td>\n",
       "      <td>0.222551</td>\n",
       "      <td>0.090122</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>455 rows × 31 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Initialization",
   "id": "256912d21714e353"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-22T15:47:20.926479Z",
     "start_time": "2025-01-22T15:47:20.917852Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "def init(data, hidden_layer_nb=2, outputs_nb=2, weights_initializer='heUniform', hidden_nodes_nb=None):\n",
    "    X_train = data.iloc[:, 1:]\n",
    "    y_train = data.iloc[:, 0]\n",
    "    # One-hot encoding with 1 and 0\n",
    "    y_train = pd.get_dummies(y_train).values\n",
    "    \n",
    "    if hidden_nodes_nb is None:\n",
    "        hidden_nodes_nb = int(((X_train.shape[1] + outputs_nb) / 2))\n",
    "\n",
    "    weights = []\n",
    "    biases = []\n",
    "    \n",
    "    for layer in range(hidden_layer_nb + 1):\n",
    "        # Input Layer to Hidden Layer\n",
    "        if layer == 0:\n",
    "            nodes_in = X_train.shape[1]\n",
    "            nodes_out = hidden_nodes_nb\n",
    "        # Hidden Layer to Hidden Layer\n",
    "        elif layer < hidden_layer_nb:\n",
    "            nodes_in = hidden_nodes_nb\n",
    "            nodes_out = hidden_nodes_nb\n",
    "        # Hidden Layer to Output Layer\n",
    "        else:\n",
    "            nodes_in = hidden_nodes_nb\n",
    "            nodes_out = outputs_nb\n",
    "            \n",
    "        if weights_initializer == 'xavier':\n",
    "            limit = np.sqrt(6 / (nodes_in + nodes_out))\n",
    "        else:\n",
    "            limit = np.sqrt(6 / nodes_in)\n",
    "        weights.append(np.random.uniform(-limit, limit, (nodes_out, nodes_in)))\n",
    "        biases.append(np.zeros(nodes_out))\n",
    "    \n",
    "    return hidden_nodes_nb, weights, biases, X_train, y_train\n",
    "    \n",
    "hidden_nodes_nb, weights, biases, X_train, y_train = init(train_data)\n",
    "\n",
    "for i in range(len(weights)):\n",
    "    print(f'weights[{i}]: {weights[i].shape}')\n",
    "    print(f'biases[{i}]: {biases[i].shape}\\n')\n"
   ],
   "id": "f5014a2046a7bf25",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights[0]: (16, 30)\n",
      "biases[0]: (16,)\n",
      "\n",
      "weights[1]: (16, 16)\n",
      "biases[1]: (16,)\n",
      "\n",
      "weights[2]: (2, 16)\n",
      "biases[2]: (2,)\n",
      "\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Activation function",
   "id": "6c68cf1c77a16878"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-22T15:47:23.495959Z",
     "start_time": "2025-01-22T15:47:23.491707Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n"
   ],
   "id": "3ce0c1d5ade0ed30",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Loss function",
   "id": "3aff59d41c64a792"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-22T15:47:25.145604Z",
     "start_time": "2025-01-22T15:47:25.141176Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def evaluate(y_train, y_pred, loss='binary_cross_entropy'):\n",
    "    if loss == 'binary_cross_entropy':\n",
    "        epsilon = 1e-15\n",
    "        y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "        N = len(y_train)\n",
    "\n",
    "        loss = -(1/N) * np.sum(\n",
    "            y_train * np.log(y_pred) + \n",
    "            (1 - y_train) * np.log(1 - y_pred)\n",
    "        )\n",
    "    return loss"
   ],
   "id": "e216bdc13a8709f6",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Forwardpropagation",
   "id": "73cc82062c148765"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T10:14:56.678397Z",
     "start_time": "2025-01-23T10:14:56.671124Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def forward_propagation(X, weights, biases, activation='sigmoid', output_activation='softmax'):\n",
    "    layers = [X]\n",
    "    Z = []  # Store pre-activation values\n",
    "    for i in range(len(weights)):\n",
    "        z = np.dot(layers[i], weights[i].T) + biases[i]\n",
    "        Z.append(z)\n",
    "        \n",
    "        if i == len(weights) - 1:\n",
    "            if output_activation == 'softmax':\n",
    "                layers.append(pd.DataFrame(softmax(z)))\n",
    "        else:\n",
    "            if activation == 'sigmoid':\n",
    "                layers.append(pd.DataFrame(sigmoid(z)))\n",
    "    return layers, Z"
   ],
   "id": "738dd7e41ba8098d",
   "outputs": [],
   "execution_count": 44
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Backpropagation\n",
   "id": "95aad347318c1429"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T10:14:58.136876Z",
     "start_time": "2025-01-23T10:14:58.131411Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def backward_propagation(y_true, activations, Z, weights):\n",
    "    gradients = {\"dW\": [], \"db\": []}\n",
    "    num_layers = len(weights)\n",
    "\n",
    "    delta = (activations[-1] - y_true) / y_true.shape[0]\n",
    "\n",
    "    for i in reversed(range(num_layers)):\n",
    "        dW = np.dot(activations[i].T, delta)\n",
    "        db = np.sum(delta, axis=0)\n",
    "        \n",
    "        gradients[\"dW\"].insert(0, dW)\n",
    "        gradients[\"db\"].insert(0, db)\n",
    "\n",
    "        if i > 0:\n",
    "            delta = np.dot(delta, weights[i].T) * sigmoid_derivative(sigmoid(Z[i-1]))  # Apply to Z instead of A\n",
    "\n",
    "    return gradients\n"
   ],
   "id": "a8c9e07093a70cc",
   "outputs": [],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T10:15:00.612514Z",
     "start_time": "2025-01-23T10:15:00.608487Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def update_parameters(weights, biases, gradients, learning_rate):\n",
    "    for i in range(len(weights)):\n",
    "        weights[i] -= learning_rate * gradients[\"dW\"][i]\n",
    "        biases[i] -= learning_rate * gradients[\"db\"][i]\n",
    "    return weights, biases"
   ],
   "id": "5d6c77546b4a5f3c",
   "outputs": [],
   "execution_count": 46
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Train the model",
   "id": "f63321d7aba2ac3c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T10:19:46.547206Z",
     "start_time": "2025-01-23T10:19:46.539906Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train(train_data, hidden_layer_nb=2, output_nb = 2,  epochs=34, learning_rate=0.01, batch_size=8):\n",
    "    \n",
    "    # Initialize weights/biases for variable hidden layers + output\n",
    "    hidden_nodes_nb, weights, biases, X_train, y_train = init(train_data, hidden_layer_nb, output_nb)\n",
    "\n",
    "    n_samples = X_train.shape[0]\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # # Shuffle data\n",
    "        # permutation = np.random.permutation(n_samples)\n",
    "        # print('x', X_train)\n",
    "        # X_shuffled = X_train[permutation]\n",
    "        # y_shuffled = y_train[permutation]\n",
    "\n",
    "        epoch_loss = 0\n",
    "\n",
    "        for i in range(0, n_samples, batch_size):\n",
    "            X_batch = X_train[i:i+batch_size]\n",
    "            y_batch = y_train[i:i+batch_size]\n",
    "\n",
    "            # Forward pass\n",
    "            activations, Z = forward_propagation(X_batch, weights, biases)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = evaluate(activations[-1], y_batch)\n",
    "            epoch_loss += loss\n",
    "\n",
    "            # Backward pass\n",
    "            gradients = backward_propagation(y_batch, activations, Z, weights)\n",
    "\n",
    "            # Update parameters\n",
    "            weights, biases = update_parameters(weights, biases, gradients, learning_rate)\n",
    "\n",
    "        # Print loss every 100 epochs\n",
    "        if epoch % 100 == 0:\n",
    "            avg_loss = epoch_loss / (n_samples // batch_size)\n",
    "            print(f\"Epoch {epoch}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    return weights, biases"
   ],
   "id": "1fae49cb1ea139d6",
   "outputs": [],
   "execution_count": 57
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T10:19:48.999539Z",
     "start_time": "2025-01-23T10:19:48.540847Z"
    }
   },
   "cell_type": "code",
   "source": "train(train_data)",
   "id": "4656a322877003db",
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "the 'keepdims' parameter is not supported in the pandas implementation of sum()",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[58], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_data\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[57], line 29\u001B[0m, in \u001B[0;36mtrain\u001B[0;34m(train_data, hidden_layer_nb, output_nb, epochs, learning_rate, batch_size)\u001B[0m\n\u001B[1;32m     26\u001B[0m epoch_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m loss\n\u001B[1;32m     28\u001B[0m \u001B[38;5;66;03m# Backward pass\u001B[39;00m\n\u001B[0;32m---> 29\u001B[0m gradients \u001B[38;5;241m=\u001B[39m \u001B[43mbackward_propagation\u001B[49m\u001B[43m(\u001B[49m\u001B[43my_batch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mactivations\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mZ\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweights\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     31\u001B[0m \u001B[38;5;66;03m# Update parameters\u001B[39;00m\n\u001B[1;32m     32\u001B[0m weights, biases \u001B[38;5;241m=\u001B[39m update_parameters(weights, biases, gradients, learning_rate)\n",
      "Cell \u001B[0;32mIn[45], line 9\u001B[0m, in \u001B[0;36mbackward_propagation\u001B[0;34m(y_true, activations, Z, weights)\u001B[0m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mreversed\u001B[39m(\u001B[38;5;28mrange\u001B[39m(num_layers)):\n\u001B[1;32m      8\u001B[0m     dW \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mdot(activations[i]\u001B[38;5;241m.\u001B[39mT, delta)\n\u001B[0;32m----> 9\u001B[0m     db \u001B[38;5;241m=\u001B[39m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msum\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdelta\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkeepdims\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m     11\u001B[0m     gradients[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdW\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39minsert(\u001B[38;5;241m0\u001B[39m, dW)\n\u001B[1;32m     12\u001B[0m     gradients[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdb\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39minsert(\u001B[38;5;241m0\u001B[39m, db)\n",
      "File \u001B[0;32m<__array_function__ internals>:200\u001B[0m, in \u001B[0;36msum\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
      "File \u001B[0;32m~/Library/Python/3.9/lib/python/site-packages/numpy/core/fromnumeric.py:2324\u001B[0m, in \u001B[0;36msum\u001B[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001B[0m\n\u001B[1;32m   2321\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m out\n\u001B[1;32m   2322\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\u001B[0;32m-> 2324\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_wrapreduction\u001B[49m\u001B[43m(\u001B[49m\u001B[43ma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43madd\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43msum\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mout\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkeepdims\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkeepdims\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2325\u001B[0m \u001B[43m                      \u001B[49m\u001B[43minitial\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minitial\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mwhere\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mwhere\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Library/Python/3.9/lib/python/site-packages/numpy/core/fromnumeric.py:84\u001B[0m, in \u001B[0;36m_wrapreduction\u001B[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001B[0m\n\u001B[1;32m     82\u001B[0m             \u001B[38;5;28;01mreturn\u001B[39;00m reduction(axis\u001B[38;5;241m=\u001B[39maxis, dtype\u001B[38;5;241m=\u001B[39mdtype, out\u001B[38;5;241m=\u001B[39mout, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mpasskwargs)\n\u001B[1;32m     83\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 84\u001B[0m             \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mreduction\u001B[49m\u001B[43m(\u001B[49m\u001B[43maxis\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43maxis\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mout\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mpasskwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     86\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m ufunc\u001B[38;5;241m.\u001B[39mreduce(obj, axis, dtype, out, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mpasskwargs)\n",
      "File \u001B[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/generic.py:11519\u001B[0m, in \u001B[0;36mNDFrame._add_numeric_operations.<locals>.sum\u001B[0;34m(self, axis, skipna, numeric_only, min_count, **kwargs)\u001B[0m\n\u001B[1;32m  11500\u001B[0m \u001B[38;5;129m@doc\u001B[39m(  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m  11501\u001B[0m     _num_doc,\n\u001B[1;32m  11502\u001B[0m     desc\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mReturn the sum of the values over the requested axis.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m  11517\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[1;32m  11518\u001B[0m ):\n\u001B[0;32m> 11519\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mNDFrame\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msum\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mskipna\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnumeric_only\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmin_count\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/generic.py:11287\u001B[0m, in \u001B[0;36mNDFrame.sum\u001B[0;34m(self, axis, skipna, numeric_only, min_count, **kwargs)\u001B[0m\n\u001B[1;32m  11279\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21msum\u001B[39m(\n\u001B[1;32m  11280\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m  11281\u001B[0m     axis: Axis \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m  11285\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[1;32m  11286\u001B[0m ):\n\u001B[0;32m> 11287\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_min_count_stat_function\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m  11288\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43msum\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnanops\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnansum\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mskipna\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnumeric_only\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmin_count\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\n\u001B[1;32m  11289\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/generic.py:11259\u001B[0m, in \u001B[0;36mNDFrame._min_count_stat_function\u001B[0;34m(self, name, func, axis, skipna, numeric_only, min_count, **kwargs)\u001B[0m\n\u001B[1;32m  11247\u001B[0m \u001B[38;5;129m@final\u001B[39m\n\u001B[1;32m  11248\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m_min_count_stat_function\u001B[39m(\n\u001B[1;32m  11249\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m  11256\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[1;32m  11257\u001B[0m ):\n\u001B[1;32m  11258\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msum\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m> 11259\u001B[0m         \u001B[43mnv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvalidate_sum\u001B[49m\u001B[43m(\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m  11260\u001B[0m     \u001B[38;5;28;01melif\u001B[39;00m name \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mprod\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m  11261\u001B[0m         nv\u001B[38;5;241m.\u001B[39mvalidate_prod((), kwargs)\n",
      "File \u001B[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/compat/numpy/function.py:82\u001B[0m, in \u001B[0;36mCompatValidator.__call__\u001B[0;34m(self, args, kwargs, fname, max_fname_arg_count, method)\u001B[0m\n\u001B[1;32m     80\u001B[0m     validate_kwargs(fname, kwargs, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdefaults)\n\u001B[1;32m     81\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m method \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mboth\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m---> 82\u001B[0m     \u001B[43mvalidate_args_and_kwargs\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     83\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_fname_arg_count\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdefaults\u001B[49m\n\u001B[1;32m     84\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     85\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     86\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minvalid validation method \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmethod\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/util/_validators.py:221\u001B[0m, in \u001B[0;36mvalidate_args_and_kwargs\u001B[0;34m(fname, args, kwargs, max_fname_arg_count, compat_args)\u001B[0m\n\u001B[1;32m    216\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\n\u001B[1;32m    217\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m() got multiple values for keyword argument \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mkey\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    218\u001B[0m         )\n\u001B[1;32m    220\u001B[0m kwargs\u001B[38;5;241m.\u001B[39mupdate(args_dict)\n\u001B[0;32m--> 221\u001B[0m \u001B[43mvalidate_kwargs\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcompat_args\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/util/_validators.py:163\u001B[0m, in \u001B[0;36mvalidate_kwargs\u001B[0;34m(fname, kwargs, compat_args)\u001B[0m\n\u001B[1;32m    161\u001B[0m kwds \u001B[38;5;241m=\u001B[39m kwargs\u001B[38;5;241m.\u001B[39mcopy()\n\u001B[1;32m    162\u001B[0m _check_for_invalid_keys(fname, kwargs, compat_args)\n\u001B[0;32m--> 163\u001B[0m \u001B[43m_check_for_default_values\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwds\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcompat_args\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/util/_validators.py:79\u001B[0m, in \u001B[0;36m_check_for_default_values\u001B[0;34m(fname, arg_val_dict, compat_args)\u001B[0m\n\u001B[1;32m     76\u001B[0m     match \u001B[38;5;241m=\u001B[39m arg_val_dict[key] \u001B[38;5;129;01mis\u001B[39;00m compat_args[key]\n\u001B[1;32m     78\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m match:\n\u001B[0;32m---> 79\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m     80\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mthe \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mkey\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m parameter is not supported in \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     81\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mthe pandas implementation of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m()\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     82\u001B[0m     )\n",
      "\u001B[0;31mValueError\u001B[0m: the 'keepdims' parameter is not supported in the pandas implementation of sum()"
     ]
    }
   ],
   "execution_count": 58
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
