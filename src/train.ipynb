{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-23T13:37:20.920924Z",
     "start_time": "2025-01-23T13:37:20.897940Z"
    }
   },
   "source": [
    "import math\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "train_data = pd.read_csv('../data/processed/train_data.csv')\n",
    "\n",
    "train_data"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "       1         2         3         4         5         6         7  \\\n",
       "0    0.0  0.096928  0.257694  0.103656  0.045387  0.487226  0.373965   \n",
       "1    1.0  0.667755  0.570172  0.683505  0.495228  0.554934  0.809214   \n",
       "2    0.0  0.103744  0.140345  0.106489  0.049799  0.221901  0.208975   \n",
       "3    0.0  0.173648  0.524518  0.167369  0.086320  0.396678  0.162444   \n",
       "4    0.0  0.150930  0.174839  0.143459  0.071432  0.548614  0.187811   \n",
       "..   ...       ...       ...       ...       ...       ...       ...   \n",
       "450  0.0  0.090255  0.166723  0.103656  0.042630  0.408053  0.410159   \n",
       "451  0.0  0.220503  0.291512  0.216847  0.114104  0.555836  0.252500   \n",
       "452  0.0  0.345923  0.240446  0.321401  0.207466  0.105263  0.022606   \n",
       "453  1.0  0.331251  0.335137  0.327068  0.193425  0.481809  0.288080   \n",
       "454  0.0  0.246060  0.365573  0.231014  0.133701  0.248262  0.064413   \n",
       "\n",
       "            8         9        10  ...        22        23        24  \\\n",
       "0    0.733365  0.217445  0.530808  ...  0.084667  0.283316  0.075153   \n",
       "1    0.582709  0.743539  0.674242  ...  0.667022  0.571962  0.627970   \n",
       "2    0.140300  0.108350  0.646970  ...  0.073995  0.192164  0.075601   \n",
       "3    0.055740  0.080268  0.422727  ...  0.153682  0.617537  0.137308   \n",
       "4    0.025398  0.064115  0.850000  ...  0.109925  0.144723  0.096867   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "450  0.201640  0.142744  0.425253  ...  0.064141  0.097281  0.060511   \n",
       "451  0.165651  0.173211  0.374242  ...  0.185343  0.459488  0.174810   \n",
       "452  0.016987  0.031064  0.226263  ...  0.248310  0.230011  0.219284   \n",
       "453  0.263824  0.321223  0.307576  ...  0.324084  0.500533  0.316201   \n",
       "454  0.055834  0.087972  0.342929  ...  0.192458  0.554904  0.170178   \n",
       "\n",
       "           25        26        27        28        29        30        31  \n",
       "0    0.034285  0.508684  0.397018  1.000000  0.601375  0.524936  0.409681  \n",
       "1    0.467902  0.514627  0.709327  0.541534  0.997595  0.499310  0.481175  \n",
       "2    0.030697  0.179555  0.136324  0.111581  0.174811  0.338459  0.195855  \n",
       "3    0.066482  0.519910  0.109158  0.089856  0.210859  0.363493  0.173357  \n",
       "4    0.045075  0.371987  0.069244  0.017316  0.088625  0.392667  0.165027  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "450  0.024381  0.327082  0.209865  0.114537  0.164467  0.135817  0.349993  \n",
       "451  0.082703  0.644720  0.231598  0.229473  0.418557  0.244628  0.235668  \n",
       "452  0.122739  0.095754  0.022383  0.030879  0.114536  0.176030  0.040404  \n",
       "453  0.168133  0.595192  0.319692  0.325000  0.627835  0.318155  0.330972  \n",
       "454  0.089117  0.271611  0.059503  0.091454  0.255361  0.222551  0.090122  \n",
       "\n",
       "[455 rows x 31 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.096928</td>\n",
       "      <td>0.257694</td>\n",
       "      <td>0.103656</td>\n",
       "      <td>0.045387</td>\n",
       "      <td>0.487226</td>\n",
       "      <td>0.373965</td>\n",
       "      <td>0.733365</td>\n",
       "      <td>0.217445</td>\n",
       "      <td>0.530808</td>\n",
       "      <td>...</td>\n",
       "      <td>0.084667</td>\n",
       "      <td>0.283316</td>\n",
       "      <td>0.075153</td>\n",
       "      <td>0.034285</td>\n",
       "      <td>0.508684</td>\n",
       "      <td>0.397018</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.601375</td>\n",
       "      <td>0.524936</td>\n",
       "      <td>0.409681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.667755</td>\n",
       "      <td>0.570172</td>\n",
       "      <td>0.683505</td>\n",
       "      <td>0.495228</td>\n",
       "      <td>0.554934</td>\n",
       "      <td>0.809214</td>\n",
       "      <td>0.582709</td>\n",
       "      <td>0.743539</td>\n",
       "      <td>0.674242</td>\n",
       "      <td>...</td>\n",
       "      <td>0.667022</td>\n",
       "      <td>0.571962</td>\n",
       "      <td>0.627970</td>\n",
       "      <td>0.467902</td>\n",
       "      <td>0.514627</td>\n",
       "      <td>0.709327</td>\n",
       "      <td>0.541534</td>\n",
       "      <td>0.997595</td>\n",
       "      <td>0.499310</td>\n",
       "      <td>0.481175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.103744</td>\n",
       "      <td>0.140345</td>\n",
       "      <td>0.106489</td>\n",
       "      <td>0.049799</td>\n",
       "      <td>0.221901</td>\n",
       "      <td>0.208975</td>\n",
       "      <td>0.140300</td>\n",
       "      <td>0.108350</td>\n",
       "      <td>0.646970</td>\n",
       "      <td>...</td>\n",
       "      <td>0.073995</td>\n",
       "      <td>0.192164</td>\n",
       "      <td>0.075601</td>\n",
       "      <td>0.030697</td>\n",
       "      <td>0.179555</td>\n",
       "      <td>0.136324</td>\n",
       "      <td>0.111581</td>\n",
       "      <td>0.174811</td>\n",
       "      <td>0.338459</td>\n",
       "      <td>0.195855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.173648</td>\n",
       "      <td>0.524518</td>\n",
       "      <td>0.167369</td>\n",
       "      <td>0.086320</td>\n",
       "      <td>0.396678</td>\n",
       "      <td>0.162444</td>\n",
       "      <td>0.055740</td>\n",
       "      <td>0.080268</td>\n",
       "      <td>0.422727</td>\n",
       "      <td>...</td>\n",
       "      <td>0.153682</td>\n",
       "      <td>0.617537</td>\n",
       "      <td>0.137308</td>\n",
       "      <td>0.066482</td>\n",
       "      <td>0.519910</td>\n",
       "      <td>0.109158</td>\n",
       "      <td>0.089856</td>\n",
       "      <td>0.210859</td>\n",
       "      <td>0.363493</td>\n",
       "      <td>0.173357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.150930</td>\n",
       "      <td>0.174839</td>\n",
       "      <td>0.143459</td>\n",
       "      <td>0.071432</td>\n",
       "      <td>0.548614</td>\n",
       "      <td>0.187811</td>\n",
       "      <td>0.025398</td>\n",
       "      <td>0.064115</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.109925</td>\n",
       "      <td>0.144723</td>\n",
       "      <td>0.096867</td>\n",
       "      <td>0.045075</td>\n",
       "      <td>0.371987</td>\n",
       "      <td>0.069244</td>\n",
       "      <td>0.017316</td>\n",
       "      <td>0.088625</td>\n",
       "      <td>0.392667</td>\n",
       "      <td>0.165027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>450</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.090255</td>\n",
       "      <td>0.166723</td>\n",
       "      <td>0.103656</td>\n",
       "      <td>0.042630</td>\n",
       "      <td>0.408053</td>\n",
       "      <td>0.410159</td>\n",
       "      <td>0.201640</td>\n",
       "      <td>0.142744</td>\n",
       "      <td>0.425253</td>\n",
       "      <td>...</td>\n",
       "      <td>0.064141</td>\n",
       "      <td>0.097281</td>\n",
       "      <td>0.060511</td>\n",
       "      <td>0.024381</td>\n",
       "      <td>0.327082</td>\n",
       "      <td>0.209865</td>\n",
       "      <td>0.114537</td>\n",
       "      <td>0.164467</td>\n",
       "      <td>0.135817</td>\n",
       "      <td>0.349993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>451</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.220503</td>\n",
       "      <td>0.291512</td>\n",
       "      <td>0.216847</td>\n",
       "      <td>0.114104</td>\n",
       "      <td>0.555836</td>\n",
       "      <td>0.252500</td>\n",
       "      <td>0.165651</td>\n",
       "      <td>0.173211</td>\n",
       "      <td>0.374242</td>\n",
       "      <td>...</td>\n",
       "      <td>0.185343</td>\n",
       "      <td>0.459488</td>\n",
       "      <td>0.174810</td>\n",
       "      <td>0.082703</td>\n",
       "      <td>0.644720</td>\n",
       "      <td>0.231598</td>\n",
       "      <td>0.229473</td>\n",
       "      <td>0.418557</td>\n",
       "      <td>0.244628</td>\n",
       "      <td>0.235668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>452</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.345923</td>\n",
       "      <td>0.240446</td>\n",
       "      <td>0.321401</td>\n",
       "      <td>0.207466</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>0.022606</td>\n",
       "      <td>0.016987</td>\n",
       "      <td>0.031064</td>\n",
       "      <td>0.226263</td>\n",
       "      <td>...</td>\n",
       "      <td>0.248310</td>\n",
       "      <td>0.230011</td>\n",
       "      <td>0.219284</td>\n",
       "      <td>0.122739</td>\n",
       "      <td>0.095754</td>\n",
       "      <td>0.022383</td>\n",
       "      <td>0.030879</td>\n",
       "      <td>0.114536</td>\n",
       "      <td>0.176030</td>\n",
       "      <td>0.040404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>453</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.331251</td>\n",
       "      <td>0.335137</td>\n",
       "      <td>0.327068</td>\n",
       "      <td>0.193425</td>\n",
       "      <td>0.481809</td>\n",
       "      <td>0.288080</td>\n",
       "      <td>0.263824</td>\n",
       "      <td>0.321223</td>\n",
       "      <td>0.307576</td>\n",
       "      <td>...</td>\n",
       "      <td>0.324084</td>\n",
       "      <td>0.500533</td>\n",
       "      <td>0.316201</td>\n",
       "      <td>0.168133</td>\n",
       "      <td>0.595192</td>\n",
       "      <td>0.319692</td>\n",
       "      <td>0.325000</td>\n",
       "      <td>0.627835</td>\n",
       "      <td>0.318155</td>\n",
       "      <td>0.330972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>454</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.246060</td>\n",
       "      <td>0.365573</td>\n",
       "      <td>0.231014</td>\n",
       "      <td>0.133701</td>\n",
       "      <td>0.248262</td>\n",
       "      <td>0.064413</td>\n",
       "      <td>0.055834</td>\n",
       "      <td>0.087972</td>\n",
       "      <td>0.342929</td>\n",
       "      <td>...</td>\n",
       "      <td>0.192458</td>\n",
       "      <td>0.554904</td>\n",
       "      <td>0.170178</td>\n",
       "      <td>0.089117</td>\n",
       "      <td>0.271611</td>\n",
       "      <td>0.059503</td>\n",
       "      <td>0.091454</td>\n",
       "      <td>0.255361</td>\n",
       "      <td>0.222551</td>\n",
       "      <td>0.090122</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>455 rows × 31 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Initialization",
   "id": "256912d21714e353"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T13:37:20.929449Z",
     "start_time": "2025-01-23T13:37:20.922719Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "def init(data, hidden_layer_nb=2, outputs_nb=2, weights_initializer='heUniform', hidden_nodes_nb=None):\n",
    "    X_train = data.iloc[:, 1:]\n",
    "    y_train = data.iloc[:, 0]\n",
    "    # One-hot encoding with 1 and 0\n",
    "    y_train = pd.get_dummies(y_train).values\n",
    "    \n",
    "    if hidden_nodes_nb is None:\n",
    "        hidden_nodes_nb = int(((X_train.shape[1] + outputs_nb) / 2))\n",
    "\n",
    "    weights = []\n",
    "    biases = []\n",
    "    \n",
    "    for layer in range(hidden_layer_nb + 1):\n",
    "        # Input Layer to Hidden Layer\n",
    "        if layer == 0:\n",
    "            nodes_in = X_train.shape[1]\n",
    "            nodes_out = hidden_nodes_nb\n",
    "        # Hidden Layer to Hidden Layer\n",
    "        elif layer < hidden_layer_nb:\n",
    "            nodes_in = hidden_nodes_nb\n",
    "            nodes_out = hidden_nodes_nb\n",
    "        # Hidden Layer to Output Layer\n",
    "        else:\n",
    "            nodes_in = hidden_nodes_nb\n",
    "            nodes_out = outputs_nb\n",
    "            \n",
    "        if weights_initializer == 'xavier':\n",
    "            limit = np.sqrt(6 / (nodes_in + nodes_out))\n",
    "        else:\n",
    "            limit = np.sqrt(6 / nodes_in)\n",
    "        weights.append(np.random.uniform(-limit, limit, (nodes_out, nodes_in)))\n",
    "        biases.append(np.zeros(nodes_out))\n",
    "    \n",
    "    return hidden_nodes_nb, weights, biases, X_train, y_train\n",
    "    \n",
    "hidden_nodes_nb, weights, biases, X_train, y_train = init(train_data)\n",
    "\n",
    "for i in range(len(weights)):\n",
    "    print(f'weights[{i}]: {weights[i].shape}')\n",
    "    print(f'biases[{i}]: {biases[i].shape}\\n')\n"
   ],
   "id": "f5014a2046a7bf25",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights[0]: (16, 30)\n",
      "biases[0]: (16,)\n",
      "\n",
      "weights[1]: (16, 16)\n",
      "biases[1]: (16,)\n",
      "\n",
      "weights[2]: (2, 16)\n",
      "biases[2]: (2,)\n",
      "\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Activation function",
   "id": "6c68cf1c77a16878"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T13:58:25.389538Z",
     "start_time": "2025-01-23T13:58:25.385238Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n"
   ],
   "id": "3ce0c1d5ade0ed30",
   "outputs": [],
   "execution_count": 45
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Loss function",
   "id": "3aff59d41c64a792"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T13:58:27.654995Z",
     "start_time": "2025-01-23T13:58:27.650151Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def evaluate(y_train, y_pred, loss='binary_cross_entropy'):\n",
    "    if loss == 'binary_cross_entropy':\n",
    "        epsilon = 1e-15\n",
    "        y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "        # N = y_train.shape[0]\n",
    "        N = len(y_train)\n",
    "\n",
    "        loss = -(1/N) * np.sum(\n",
    "            y_train * np.log(y_pred) + \n",
    "            (1 - y_train) * np.log(1 - y_pred)\n",
    "        )\n",
    "    return loss"
   ],
   "id": "e216bdc13a8709f6",
   "outputs": [],
   "execution_count": 46
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Forwardpropagation",
   "id": "73cc82062c148765"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T13:58:29.237423Z",
     "start_time": "2025-01-23T13:58:29.231819Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def forward_propagation(X, weights, biases, activation='sigmoid', output_activation='softmax'):\n",
    "    layers = [X]  # List of layer activations\n",
    "    Z = []        # List of pre-activation values\n",
    "    \n",
    "    for i in range(len(weights)):\n",
    "        z = np.dot(layers[i], weights[i].T) + biases[i]\n",
    "        Z.append(z)\n",
    "\n",
    "        # Compute activation\n",
    "        if i == len(weights) - 1:\n",
    "            if output_activation == 'softmax':\n",
    "                activation_output = softmax(z)\n",
    "            else:\n",
    "                activation_output = sigmoid(z)\n",
    "        else:\n",
    "            if activation == 'sigmoid':\n",
    "                activation_output = sigmoid(z)\n",
    "        \n",
    "        layers.append(activation_output)\n",
    "\n",
    "    return layers, Z"
   ],
   "id": "738dd7e41ba8098d",
   "outputs": [],
   "execution_count": 47
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Backpropagation\n",
   "id": "95aad347318c1429"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T13:58:32.802571Z",
     "start_time": "2025-01-23T13:58:32.797435Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def backward_propagation(y_true, activations, Z, weights):\n",
    "    gradients = {\"dW\": [], \"db\": []}\n",
    "    num_layers = len(weights)\n",
    "    m = y_true.shape[0]  # Number of samples\n",
    "\n",
    "    delta = (activations[-1] - y_true) / m\n",
    "\n",
    "    for i in reversed(range(num_layers)):\n",
    "        dW = np.dot(delta.T, activations[i])\n",
    "        db = np.sum(delta, axis=0)\n",
    "        \n",
    "        gradients[\"dW\"].insert(0, dW)\n",
    "        gradients[\"db\"].insert(0, db)\n",
    "\n",
    "        if i > 0:\n",
    "            delta = np.dot(delta, weights[i]) * sigmoid_derivative(Z[i-1])  # Apply to Z instead of A\n",
    "\n",
    "    return gradients\n"
   ],
   "id": "a8c9e07093a70cc",
   "outputs": [],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T13:58:34.691486Z",
     "start_time": "2025-01-23T13:58:34.686012Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def update_parameters(weights, biases, gradients, learning_rate):\n",
    "    for i in range(len(weights)):\n",
    "        weights[i] -= learning_rate * gradients[\"dW\"][i]\n",
    "        biases[i] -= learning_rate * gradients[\"db\"][i]\n",
    "    return weights, biases"
   ],
   "id": "5d6c77546b4a5f3c",
   "outputs": [],
   "execution_count": 49
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Train the model",
   "id": "f63321d7aba2ac3c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T13:58:40.312885Z",
     "start_time": "2025-01-23T13:58:36.942255Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train(train_data, hidden_layer_nb=2, output_nb = 2,  epochs=1000, learning_rate=0.005, batch_size=8, patience_early_stop=5):\n",
    "    \n",
    "    # Initialize weights/biases for variable hidden layers + output\n",
    "    hidden_nodes_nb, weights, biases, X_train, y_train = init(train_data, hidden_layer_nb, output_nb, 'xavier')\n",
    "\n",
    "    n_samples = X_train.shape[0]\n",
    "    wait = 0\n",
    "    best_loss = float('inf')\n",
    "    for epoch in range(epochs):\n",
    "        # Shuffle data\n",
    "        permutation = np.random.permutation(n_samples)\n",
    "        X_shuffled = X_train.iloc[permutation]\n",
    "        y_shuffled = y_train[permutation]\n",
    "        \n",
    "        epoch_loss = 0\n",
    "\n",
    "        # Mini-batch training\n",
    "        for i in range(0, n_samples, batch_size):\n",
    "            X_batch = X_shuffled.iloc[i:i+batch_size]\n",
    "            y_batch = y_shuffled[i:i+batch_size]\n",
    "\n",
    "            # Forward pass\n",
    "            activations, Z = forward_propagation(X_batch, weights, biases)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = round(evaluate(y_batch, activations[-1]), 4)\n",
    "\n",
    "            epoch_loss += loss\n",
    "\n",
    "            # Backward pass\n",
    "            gradients = backward_propagation(y_batch, activations, Z, weights)\n",
    "\n",
    "            # Update parameters\n",
    "            weights, biases = update_parameters(weights, biases, gradients, learning_rate)\n",
    "\n",
    "        # Early stopping\n",
    "        avg_loss = epoch_loss / (n_samples // batch_size)\n",
    "        if avg_loss < best_loss:\n",
    "            best_loss = avg_loss\n",
    "            wait = 0\n",
    "        else:\n",
    "            wait += 1\n",
    "            if wait >= patience_early_stop:\n",
    "                print(f\"Early stopping at epoch {epoch}\")\n",
    "                break\n",
    "        \n",
    "        # Print loss every 100 epochs\n",
    "        if epoch % 100 == 0:\n",
    "            avg_loss = epoch_loss / (n_samples // batch_size)\n",
    "            print(f\"Epoch {epoch}, Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    return weights, biases\n",
    "\n",
    "weights, biases = train(train_data)\n",
    "# Save weights and biases\n",
    "weights = np.array(weights, dtype=object)\n",
    "biases = np.array(biases, dtype=object)\n",
    "np.save('../data/trained/weights.npy', weights)\n",
    "np.save('../data/trained/biases.npy', biases)"
   ],
   "id": "1fae49cb1ea139d6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 1.3788\n",
      "Epoch 100, Loss: 1.2502\n",
      "Epoch 200, Loss: 0.7631\n",
      "Epoch 300, Loss: 0.3886\n",
      "Epoch 400, Loss: 0.2800\n",
      "Epoch 500, Loss: 0.2306\n",
      "Epoch 600, Loss: 0.2016\n",
      "Epoch 700, Loss: 0.1830\n",
      "Epoch 800, Loss: 0.1700\n",
      "Early stopping at epoch 815\n"
     ]
    }
   ],
   "execution_count": 50
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
